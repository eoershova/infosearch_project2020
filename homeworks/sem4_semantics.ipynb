{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать реализацию Word2vec в библиотеке **Gensim**, а в качестве предобученных моделей возьмем модели Андрея Кутузова и Лизы Кузьменко с сайта [RusVectōrēs.](https://rusvectores.org/ru/models/). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей давайте возьмем \n",
    "\n",
    "1) araneum_none_fasttextcbow_300_5_2018 (fasttext) - модель, обученная на интернет-корпусе русского языка\n",
    "\n",
    "\n",
    "2) ruscorpora_upos_skipgram_300_5_2018 (word2vec) - модель, обученная НКРЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec + fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют несколько форматов, в которых могут храниться модели - .vec и .model \n",
    "\n",
    "1) Первый формат считается классическим вариантом модели word2vec. Для загрузки таакой модели надо воспользоваться методом *KeyedVectors.load_word2vec_format*. \n",
    "Модель может быть бинарной, для ее загрузки надо передать параметр binary, равный True. \n",
    "\n",
    "2) Формат .model - собственный формат gensim. Такую модель надо загружать с помощью метода *KeyedVectors.load*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1) если модель без тэгов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка модели\n",
    "\n",
    "model_file = '../../data/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model = KeyedVectors.load(model_file)\n",
    "\n",
    "\n",
    "#проверка наличия слова в словаре\n",
    "\n",
    "lemma = 'заграница'\n",
    "lemma in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) если модель с POS-тэггингом**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка модели\n",
    "\n",
    "# model_file = '../../data/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "# model_POS = KeyedVectors.load_word2vec_format(model_file, binary=False)\n",
    "\n",
    "\n",
    "#проверка наличия слова в словаре\n",
    "\n",
    "lemma = 'заграница_NOUN'\n",
    "lemma in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) получение вектора слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = 'заграница'\n",
    "lemma in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['заграница']\n",
    "v2 = model_POS.wv['заграница_NOUN']\n",
    "\n",
    "(v1 == v2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение вектора документа\n",
    "\n",
    "Отлично, вектора для слов получены. Что с ними делать дальше? \n",
    "\n",
    "Есть два подхода (а точнее есть один, а второй мы придумали, потому что с одним жить нельзя).\n",
    "> Классика - для получения вектора документа нужно взять и усреднить все вектора его слов\n",
    " \n",
    "$$ vec_{doc} = \\frac {\\sum_{i=0}^{n} vec_i}{len(d)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# сделали препроцессинг, получили леммы \n",
    "lemmas = ['старинный', 'замок']\n",
    "\n",
    "# создаем вектор-маску\n",
    "lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "vec = np.zeros((model.vector_size,))\n",
    "\n",
    "# если слово есть в модели, берем его вектор\n",
    "for idx, lemma in enumerate(lemmas):\n",
    "    if lemma in model:\n",
    "        lemmas_vectors[idx] = model[lemma]\n",
    "        \n",
    "# проверка на случай, если на вход пришел пустой массив\n",
    "if lemmas_vectors.shape[0] is not 0:\n",
    "    vec = np.mean(lemmas_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Эксперимент - представим документ не в виде одного уредненного вектора, а как матрицу векторов входящих в него слов\n",
    "\n",
    "```\n",
    " слово1 |  v1_300\n",
    " слово2 |  v2_300\n",
    " слово3 |  v3_300\n",
    " слово4 |  v4_300\n",
    "```\n",
    "\n",
    "> Отлично, теперь каждый документ представлен в виде матрицы векторов своих слов. Но нам надо получить близость матрицы документа в коллекции и матрицы входящего запроса. Как? Умножим две матрицы друг на друга - одна матрица размером d x 300, другая q x 300 - получим попарную близость слов из каждого документа - матрицу размером d x q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем игрушечный пример кейса\n",
    "\n",
    "text1 = 'турция' \n",
    "text2 = 'нужна справка срочно'\n",
    "query = 'быстрая справка'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# построим матрицы всех документов\n",
    "\n",
    "def create_doc_matrix(text):\n",
    "    lemmas = text.split()\n",
    "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "    vec = np.zeros((model.vector_size,))\n",
    "\n",
    "    for idx, lemma in enumerate(lemmas):\n",
    "        if lemma in model.wv:\n",
    "            lemmas_vectors[idx] = normalize_vec(model.wv[lemma])\n",
    "            \n",
    "    return lemmas_vectors    \n",
    "\n",
    "\n",
    "text1_m = create_doc_matrix(text1)\n",
    "text2_m = create_doc_matrix(text2)\n",
    "query_m = create_doc_matrix(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размер матрицы как и ожидали\n",
    "query_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09587915, 0.01183069]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на близость слов первого текста и слов запроса\n",
    "text1_m.dot(query_m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0260624 ,  0.11607588],\n",
       "       [ 0.01341236,  1.00000011],\n",
       "       [ 0.22505549,  0.33582122]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на близость слов второго текста и слов запроса\n",
    "text2_m.dot(query_m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10770983955697251, 1.225055597288777]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_m = [text1_m, text2_m]\n",
    "\n",
    "    \n",
    "def search(docs, query, reduce_func=np.max, axis=0):\n",
    "    sims = []\n",
    "    for doc in docs:\n",
    "        sim = doc.dot(query.T)\n",
    "        sim = reduce_func(sim, axis=axis)\n",
    "        sims.append(sim.sum())\n",
    "    print(sims)\n",
    "    return np.argmax(sims)\n",
    "\n",
    "\n",
    "search(docs_m, query_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте поиск по нашему стандартному Covid корпусу с помощью модели на Araneum двумя способами:\n",
    "\n",
    "    1. преобразуйте каждый документ в вектор через усреднение векторов его слов и реализуйте поисковик как \n",
    "    обычно через умножение матрицы документов коллекции на вектор запроса \n",
    "    2. экспериментальный способ - реализуйте поиск ближайшего документа в коллекции к запросу, преобразовав \n",
    "    каждый документ в матрицу (количество слов x размер модели)\n",
    "    \n",
    "Посчитайте качество поиска для каждой модели на тех же данных, что и в предыдущем задании. В качестве препроцессинга используйте две версии - с удалением NER и без удаления.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Загрузка моделей и стопслов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    global model\n",
    "    model_file = 'model_files/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "    model = KeyedVectors.load(model_file)\n",
    "    \n",
    "get_model()\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading perluniprops: <urlopen error [Errno 8]\n",
      "[nltk_data]     nodename nor servname provided, or not known>\n",
      "[nltk_data] Error loading nonbreaking_prefixes: <urlopen error [Errno\n",
      "[nltk_data]     8] nodename nor servname provided, or not known>\n",
      "2020-10-21 19:28:07.663 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /Users/elizavetaersova/.deeppavlov/models/ner_rus/word.dict]\n",
      "2020-10-21 19:28:07.721 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /Users/elizavetaersova/.deeppavlov/models/ner_rus/tag.dict]\n",
      "2020-10-21 19:28:07.726 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /Users/elizavetaersova/.deeppavlov/models/ner_rus/char.dict]\n",
      "2020-10-21 19:28:07.767 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 53: [loading fastText embeddings from `/Users/elizavetaersova/.deeppavlov/downloads/embeddings/lenta_lower_100.bin`]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/models/ner/network.py:96: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/models/ner/network.py:170: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:409: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:420: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/models/ner/network.py:211: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/common/check_gpu.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 19:28:20.26 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:733: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:737: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:740: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/lstm_ops.py:360: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:865: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 19:28:22.429 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/models/ner/network.py:248: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/tensorflow_core/contrib/crf/python/ops/crf.py:99: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/models/ner/network.py:166: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:50: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 19:28:25.124 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /Users/elizavetaersova/.deeppavlov/models/ner_rus/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/elizavetaersova/.deeppavlov/models/ner_rus/model\n",
      "CPU times: user 8.6 s, sys: 3.05 s, total: 11.6 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from deeppavlov import configs, build_model\n",
    "ner_model = build_model(configs.ner.ner_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Индексирование базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Загружу и проиндексирую тексты в разных вариантах предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_base = pd.read_csv('X_train_NER_preproc.csv')\n",
    "tokens_corpus = queries_base.clean_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^а-яё]', ' ', text)\n",
    "    text = [token for token in text.split() if token not in russian_stopwords]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def normalize_vec(v):\n",
    "    return v / np.sqrt(np.sum(v ** 2))\n",
    "\n",
    "def preprocess_with_deepmipt(text: str) -> str:\n",
    "    anno = ner_model([text])\n",
    "    tokens = anno[0][0]\n",
    "    tags = anno[1][0]\n",
    "    clean_tokens = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag == 'O':\n",
    "            clean_tokens.append(token)\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "    return clean_text\n",
    "\n",
    "def query_base_indexing_w2v_basic():\n",
    "    global w2v_basic_matrix\n",
    "    vectors = []\n",
    "    for query in tokens_corpus:\n",
    "        tokens = query.split()\n",
    "        tokens_vectors = np.zeros((len(tokens), model.vector_size))\n",
    "        vec = np.zeros((model.vector_size,))\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in model:\n",
    "                tokens_vectors[idx] = model[token]\n",
    "        if tokens_vectors.shape[0] is not 0:\n",
    "            vec = np.mean(tokens_vectors, axis=0)\n",
    "        vec = normalize_vec(vec)\n",
    "        vectors.append(vec)\n",
    "    w2v_basic_matrix = np.matrix(vectors)\n",
    "\n",
    "\n",
    "def w2v_advanced_index_single_doc(text):\n",
    "    lemmas = text.split()\n",
    "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "\n",
    "    for idx, lemma in enumerate(lemmas):\n",
    "        if lemma in model:\n",
    "            lemmas_vectors[idx] = normalize_vec(model[lemma])\n",
    "    return lemmas_vectors\n",
    "\n",
    "\n",
    "def query_base_indexing_w2v_advanced():\n",
    "    global w2v_advanced_index\n",
    "    w2v_advanced_index = []\n",
    "    for query in tokens_corpus:\n",
    "        query_matrix = w2v_advanced_index_single_doc(query)\n",
    "        w2v_advanced_index.append(query_matrix)\n",
    "    return w2v_advanced_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.19 s, sys: 197 ms, total: 5.39 s\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_base_indexing_w2v_basic()\n",
    "query_base_indexing_w2v_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_basic_search(query):\n",
    "    tokens = preprocess(query).split()\n",
    "    tokens_vectors = np.zeros((len(tokens), model.vector_size))\n",
    "    vec = np.zeros((model.vector_size,))\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token in model:\n",
    "            tokens_vectors[idx] = model[token]\n",
    "    if tokens_vectors.shape[0] is not 0:\n",
    "        vec = np.mean(tokens_vectors, axis=0)\n",
    "    vec = normalize_vec(vec)\n",
    "    query_matrix = np.matrix(vec)\n",
    "    cosine_similarities = linear_kernel(query_matrix, w2v_basic_matrix).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "    answer_doc = related_docs_indices[0]\n",
    "    answer_id = queries_base['answer_id'][answer_doc]\n",
    "    return answer_id\n",
    "\n",
    "\n",
    "def w2v_advanced_search(query):\n",
    "    query = preprocess(query)\n",
    "    query_matrix = w2v_advanced_index_single_doc(query)\n",
    "    sims = []\n",
    "    for doc in w2v_advanced_index:\n",
    "        sim = doc.dot(query_matrix.T)\n",
    "        sim = np.max(sim, axis=0)\n",
    "        sims.append(sim.sum())\n",
    "    answer_doc = np.argmax(sims)\n",
    "    answer_id = queries_base['answer_id'][answer_doc]\n",
    "    return answer_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Измерю качество поиска с удалением NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test_3ner.csv')\n",
    "y_test = X_test.answer_id.tolist()\n",
    "X_test_basic = X_test.clean_text.tolist()\n",
    "X_test_ner_deepmipt = X_test.Deepmipt_NER_removal.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Стандартный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5434027777777778\n",
      "CPU times: user 2.9 s, sys: 216 ms, total: 3.12 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_corr = 0\n",
    "n_queries = len(X_test_ner_deepmipt)\n",
    "for query, answer in zip(X_test_ner_deepmipt, y_test):\n",
    "    pred_answer = w2v_basic_search(query)\n",
    "    real_answer = answer\n",
    "    if pred_answer == real_answer:\n",
    "        n_corr += 1\n",
    "print(f'Accuracy {n_corr/n_queries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Экспериментальный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5\n",
      "CPU times: user 1min 41s, sys: 5.13 s, total: 1min 46s\n",
      "Wall time: 54.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_corr = 0\n",
    "n_queries = len(X_test_ner_deepmipt)\n",
    "for query, answer in zip(X_test_ner_deepmipt, y_test):\n",
    "    pred_answer = w2v_advanced_search(query)\n",
    "    real_answer = answer\n",
    "    if pred_answer == real_answer:\n",
    "        n_corr += 1\n",
    "print(f'Accuracy {n_corr/n_queries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Измерю качество поиска без удаления NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Стандартный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5625\n",
      "CPU times: user 3.04 s, sys: 223 ms, total: 3.27 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_corr = 0\n",
    "n_queries = len(X_test_basic)\n",
    "for query, answer in zip(X_test_basic, y_test):\n",
    "    pred_answer = w2v_basic_search(query)\n",
    "    real_answer = answer\n",
    "    if pred_answer == real_answer:\n",
    "        n_corr += 1\n",
    "print(f'Accuracy {n_corr/n_queries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Экспериментальный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5138888888888888\n",
      "CPU times: user 1min 53s, sys: 5.16 s, total: 1min 58s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_corr = 0\n",
    "n_queries = len(X_test_basic)\n",
    "for query, answer in zip(X_test_basic, y_test):\n",
    "    pred_answer = w2v_advanced_search(query)\n",
    "    real_answer = answer\n",
    "    if pred_answer == real_answer:\n",
    "        n_corr += 1\n",
    "print(f'Accuracy {n_corr/n_queries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Вывод:\n",
    "    - удаление NE не улучшает качество поиска\n",
    "    - на тестовой выборке стандартный метод работает лучше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
